{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH = \"./checkpoint_9.pth\"\n",
    "VALIDATION_JSON = \"/Users/jagathkumarreddyk/Documents/GitHub/BLIP/annotations_trainval2017/annotations/captions_val2017.json\"\n",
    "VALIDATION_IMAGE_ROOT = \"/Users/jagathkumarreddyk/Documents/GitHub/BLIP/val2017/val2017\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jagathkumarreddyk/Documents/GitHub/ICapGPT/.venv/lib/python3.14/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import os\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QFormer(\n",
       "  (cross_attn): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       "  (mlp): Linear(in_features=768, out_features=768, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3,[0.5]*3)\n",
    "])\n",
    "class COCOCaptionDataset(Dataset):\n",
    "    \"\"\"COCO Captions Dataset\"\"\"\n",
    "    def __init__(self, json_path, image_root, transform, tokenizer, max_length=50):\n",
    "        with open(json_path, 'r') as f:\n",
    "            self.data = json.load(f)\n",
    "        \n",
    "        self.image_root = image_root\n",
    "        self.transform = transform\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Flatten annotations\n",
    "        self.samples = []\n",
    "        for item in tqdm(self.data['annotations']):\n",
    "            img_id = item['image_id']\n",
    "            if len(self.samples)==200: break\n",
    "            # Find image filename\n",
    "            img_info = next(img for img in self.data['images'] if img['id'] == img_id)\n",
    "            self.samples.append({\n",
    "                'image': os.path.join(image_root, img_info['file_name']),\n",
    "                'caption': item['caption']\n",
    "            })\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        \n",
    "        # Load and transform image\n",
    "        image = Image.open(sample['image']).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "        \n",
    "        # Tokenize caption for encoder\n",
    "        caption = sample['caption']\n",
    "        text_encoding = self.tokenizer(\n",
    "            caption,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Create decoder inputs (shifted right)\n",
    "        decoder_input_ids = text_encoding['input_ids'].clone()\n",
    "        labels = text_encoding['input_ids'].clone()\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "        \n",
    "        return {\n",
    "            'image': image,\n",
    "            'image_path': sample['image'],\n",
    "            'text_input_ids': text_encoding['input_ids'].squeeze(0),\n",
    "            'text_attention_mask': text_encoding['attention_mask'].squeeze(0),\n",
    "            'decoder_input_ids': decoder_input_ids.squeeze(0),\n",
    "            'decoder_attention_mask': text_encoding['attention_mask'].squeeze(0),\n",
    "            'labels': labels.squeeze(0)\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# =======================================================\n",
    "# 7. Load tokenizer + model\n",
    "# =======================================================\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Add special <img> token\n",
    "if '<img>' not in tokenizer.get_vocab():\n",
    "    tokenizer.add_special_tokens({'additional_special_tokens':['<img>']})\n",
    "img_token_id = tokenizer.convert_tokens_to_ids('<img>')\n",
    "\n",
    "# Load DistilGPT2 via AutoModelForCausalLM\n",
    "gpt2 = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
    "gpt2.resize_token_embeddings(len(tokenizer))\n",
    "gpt2.eval()\n",
    "\n",
    "# =======================================================\n",
    "# 8. Q-Former\n",
    "# =======================================================\n",
    "from transformers import ViTModel\n",
    "vit = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "vit.eval()\n",
    "\n",
    "class QFormer(nn.Module):\n",
    "    def __init__(self, image_emb_dim, prompt_len=16, hidden_dim=768):\n",
    "        super().__init__()\n",
    "        self.query_tokens = nn.Parameter(torch.randn(prompt_len, image_emb_dim))\n",
    "        self.cross_attn = nn.MultiheadAttention(embed_dim=image_emb_dim, num_heads=8)\n",
    "        self.mlp = nn.Linear(image_emb_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, image_embeds):\n",
    "        batch_size = image_embeds.size(0)\n",
    "        query = self.query_tokens.unsqueeze(1).repeat(1,batch_size,1)\n",
    "        attn_out,_ = self.cross_attn(query, image_embeds.transpose(0,1), image_embeds.transpose(0,1))\n",
    "        prompt = self.mlp(attn_out).transpose(0,1)\n",
    "        return prompt\n",
    "\n",
    "\n",
    "q_former = QFormer(image_emb_dim=vit.config.hidden_size, prompt_len=16, hidden_dim=gpt2.config.n_embd)\n",
    "\n",
    "# =======================================================\n",
    "# 9. DataLoader\n",
    "# =======================================================\n",
    "# val_dataset = COCOCaptionDataset(json_path=VALIDATION_JSON, image_root=VALIDATION_IMAGE_ROOT, transform=transform,tokenizer=tokenizer, max_length=50)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# =======================================================\n",
    "# 10. Device setup\n",
    "# =======================================================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vit.to(device)\n",
    "gpt2.to(device)\n",
    "q_former.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 2965248\n"
     ]
    }
   ],
   "source": [
    "trainable_params = sum(\n",
    "    p.numel() for p in q_former.parameters() if p.requires_grad\n",
    ")\n",
    "\n",
    "print(f\"Trainable parameters: {trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_checkpoint(CHECKPOINT_PATH):\n",
    "    # with open(CHECKPOINT_PATH, 'r') as f:\n",
    "    checkpoint_obj = torch.load(CHECKPOINT_PATH)\n",
    "    gpt2.load_state_dict(checkpoint_obj[\"gpt2_state\"])\n",
    "    q_former.load_state_dict(checkpoint_obj[\"qformer\"])\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 768])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.bos_token_id\n",
    "gpt2.transformer.wte(torch.tensor([[50256]])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81913344"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in gpt2.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2965248"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in q_former.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(pil_image, vit, q_former, gpt2, tokenizer, device, max_length=30, top_k=50, top_p=0.95):\n",
    "    vit.eval()\n",
    "    q_former.eval()\n",
    "    gpt2.eval()\n",
    "    image = transform(pil_image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        image_embeds = vit(image).last_hidden_state\n",
    "        prompts = q_former(image_embeds)\n",
    "        input_ids = torch.tensor([[tokenizer.bos_token_id]], device=device)\n",
    "        img_token_emb = gpt2.transformer.wte(torch.tensor([[img_token_id]], device=device))\n",
    "        generated = []\n",
    "\n",
    "        for _ in range(max_length):\n",
    "            gpt2_inputs = gpt2.transformer.wte(input_ids)\n",
    "            gpt2_inputs = torch.cat([img_token_emb, prompts, gpt2_inputs], dim=1)\n",
    "\n",
    "            outputs = gpt2(inputs_embeds=gpt2_inputs)\n",
    "            logits = outputs.logits[0, -1, :]\n",
    "\n",
    "            # Top-k + top-p sampling\n",
    "            filtered_logits = torch.nn.functional.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(filtered_logits, num_samples=1)\n",
    "\n",
    "            if next_token.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "            generated.append(next_token.item())\n",
    "            input_ids = torch.cat([input_ids, next_token.unsqueeze(0)], dim=1)\n",
    "\n",
    "        caption = tokenizer.decode(generated, skip_special_tokens=True)\n",
    "        return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output(num_samples, val_dataset):\n",
    "    for i in range(num_samples):\n",
    "        img = (((val_dataset[0]['image'] + 1)/2).mul(255)).byte()\n",
    "        img = img.permute(1,2,0)\n",
    "        print(img.shape)\n",
    "        img = Image.fromarray(img.numpy())\n",
    "        gt_caption = tokenizer.decode(val_dataset[i]['text_input_ids'])\n",
    "        gen_caption = generate_caption(img, vit, q_former, gpt2, tokenizer, device)\n",
    "\n",
    "        print(\"GT caption: \" + gt_caption)\n",
    "        print(\"Generated caption: \" + gen_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_output(10, val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LEt s checkout the structure of karpathy coco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Replace with your file name\n",
    "with open(\"/Users/jagathkumarreddyk/Documents/GitHub/BLIP/dataset_coco.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data[\"images\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"/Users/jagathkumarreddyk/Documents/GitHub/BLIP/dataset_coco.json\",'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "train = [img for img in data[\"images\"] if img[\"split\"] == \"train\"]\n",
    "val = [img for img in data[\"images\"] if img[\"split\"] == \"val\"]\n",
    "test = [img for img in data[\"images\"] if img[\"split\"] == \"test\"]\n",
    "\n",
    "print(len(train), len(val), len(test))\n",
    "print(len(train) + len(val) + len(test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We are not going to use karpathy split, Since it requires val2014, which we dont have\n",
    "\n",
    "SO VAL2017 it is!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_output_json = []\n",
    "\n",
    "for i in range(5):\n",
    "    this_dict = {}\n",
    "    img = (((val_dataset[i]['image'] + 1)/2).mul(255)).byte()\n",
    "    img = img.permute(1,2,0)\n",
    "    print(img.shape)\n",
    "    img = Image.fromarray(img.numpy())\n",
    "    gt_caption = remove_padding(val_dataset[i]['text_input_ids'])\n",
    "    gt_caption = tokenizer.decode(gt_caption)\n",
    "    gen_caption = generate_caption(img, vit, q_former, gpt2, tokenizer, device)\n",
    "    \n",
    "    this_dict['img_id'] = int(str(val_dataset[0]['image_path']).split(\".\")[0].split(\"/\")[-1])\n",
    "    this_dict[\"image_path\"] = val_dataset[i]['image_path']\n",
    "    this_dict[\"captions\"] = gt_caption\n",
    "    this_dict[\"generated_output\"]  = gen_caption\n",
    "    \n",
    "    sample_output_json.append(this_dict)\n",
    "\n",
    "    print(\"IMAGE_PATH\",val_dataset[i]['image_path'])\n",
    "    print(\"GT caption: \" + gt_caption)\n",
    "    print(\"Generated caption: \" + gen_caption,\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_output_json[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(gt_caption)\n",
    "for i in range()\n",
    "\n",
    "gt_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(\"<|endoftext|>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dict \n",
    "```\n",
    "{\n",
    "    id : Image_id,\n",
    "    path: file_path/fileName,\n",
    "    output: MODEL's GENERATED OUTPUT,\n",
    "    human_cpations: [\"humancaption_1\",\"humancaption_2\",\"humancaption_3\"]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(VALIDATION_JSON, 'r') as f:\n",
    "    val_dat = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dat.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dat[\"images\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dat[\"annotations\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset[0]['text_input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_dataset[0]['text_input_ids']\n",
    "def remove_padding(padded_tokens):\n",
    "    for i in range(len(padded_tokens)-1):\n",
    "        if (padded_tokens[i] == 50256) and (padded_tokens[i+1] == 50256):\n",
    "            break\n",
    "\n",
    "    return padded_tokens[:i]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_padding(val_dataset[0]['text_input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./output_json.json\", 'w') as f:\n",
    "    json.dump(sample_output_json,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/jagathkumarreddyk/Documents/GitHub/ICapGPT/output_json_1.json\", 'r') as f:\n",
    "    d = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "len(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_root = \"/Users/jagathkumarreddyk/Documents/GitHub/BLIP/val2017/val2017\"\n",
    "for img_file in os.listdir(image_root):\n",
    "    if str(179765) in img_file:\n",
    "        print(img_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.join(image_root,img_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_json(output_json_path):\n",
    "    with open(output_json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "json_path = \"/Users/jagathkumarreddyk/Downloads/CV/BLIP2-Output/output_json_3epoch_model.json\"\n",
    "d = get_json(json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "br_path = d[3]['image_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int((br_path).split(\"/\")[-1].split(\".\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_obj = torch.load(\"./checkpoint_batch_srun2999_epoch12.pth\", map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint_obj[\"gpt2_state\"]\n",
    "checkpoint_obj = torch.load(CHECKPOINT_PATH, map_location=torch.device(\"cpu\"))\n",
    "gpt2.load_state_dict(checkpoint_obj[\"gpt2_state\"])\n",
    "q_former.load_state_dict(checkpoint_obj[\"qformer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets incorporate generate output from Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pycocotools.coco import COCO\n",
    "from pycocoevalcap.eval import COCOEvalCap\n",
    "\n",
    "from pycocoevalcap.bleu.bleu import Bleu\n",
    "from pycocoevalcap.rouge.rouge import Rouge\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "\n",
    "def calculate_metrics(gt_json_path, results_json_path):\n",
    "    \"\"\"\n",
    "    Calculates standard COCO captioning metrics (BLEU, METEOR, ROUGE-L, CIDEr, SPICE).\n",
    "\n",
    "    Args:\n",
    "        gt_json_path (str): Path to the COCO Ground Truth JSON file (e.g., 'captions_val2017.json').\n",
    "        results_json_path (str): Path to the Generated Captions JSON file.\n",
    "    \"\"\"\n",
    "    # Initialize COCO for ground truth annotations\n",
    "    coco = COCO(gt_json_path)\n",
    "    \n",
    "    # Initialize COCOEvalCap\n",
    "    # The 'resFile' argument specifies the path to the generated captions\n",
    "    coco_eval = COCOEvalCap(coco, results_json_path)\n",
    "\n",
    "    coco_eval.evaluators = [\n",
    "        (str('Bleu'), Bleu(4)), # Bleu(4) calculates BLEU-1, 2, 3, 4\n",
    "        (str('Rouge'), Rouge()),\n",
    "        (str('Cider'), Cider()),\n",
    "        # Skip ('Meteor', Meteor()) and ('Spice', Spice())\n",
    "    ]\n",
    "\n",
    "    # Note: image IDs need to be a subset of the image IDs in the GT file\n",
    "    # This automatically sets the list of image IDs to evaluate based on your results file\n",
    "    # For val2017, this should be all 5k images.\n",
    "    \n",
    "    print(f\"Starting evaluation on image IDs in: {results_json_path}...\")\n",
    "    \n",
    "    # Perform the evaluation\n",
    "    # This will run all standard metrics (BLEU-1 to 4, METEOR, ROUGE-L, CIDEr, SPICE)\n",
    "    coco_eval.evaluate()\n",
    "\n",
    "    # Print the resulting scores\n",
    "    print(\"\\n--- Evaluation Scores ---\")\n",
    "    \n",
    "    # The result is a dictionary: {'Bleu_1': score, 'Bleu_2': score, ...}\n",
    "    for metric, score in coco_eval.eval.items():\n",
    "        # Format the score to a standard paper-reporting format (e.g., 2 decimal places)\n",
    "        print(f\"{metric}: {score:.3f}\")\n",
    "        \n",
    "    return coco_eval.eval\n",
    "\n",
    "# --- Configuration ---\n",
    "# You need to replace these paths with your actual file locations\n",
    "# 1. Path to the official COCO val2017 GT Captions file\n",
    "#    You must download this file from the COCO website (e.g., annotations/captions_val2017.json)\n",
    "GT_JSON_FILE = '/Users/jagathkumarreddyk/Documents/GitHub/BLIP/annotations_trainval2017/annotations/captions_val2017.json'\n",
    "# GT_JSON_FILE = \"./GROUND_TRUTH_JSON.json\"\n",
    "# 2. Path to your model's generated captions JSON file (in the format specified in Section 1B)\n",
    "RESULTS_JSON_FILE = './new_format_json'\n",
    "\n",
    "# # --- Run Evaluation ---\n",
    "\n",
    "try:\n",
    "    metrics_scores = calculate_metrics(GT_JSON_FILE, RESULTS_JSON_FILE)\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"\\n[ERROR] File not found. Please check your paths: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n[ERROR] An error occurred during evaluation. Did you install Java and all required packages (pycocotools, pycocoevalcap)? Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/jagathkumarreddyk/Downloads/CV/BLIP2-Output/output_json_12_epoch_model_nov29.json\", 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"image_id\": 391895, \"caption\": \"A man\n",
    "GROUND_TRUTH = []\n",
    "LIST_right_format = []\n",
    "for d in data:\n",
    "    temp = {}\n",
    "    gt_d  = {}\n",
    "    temp['image_id'] = d[\"img_id\"]\n",
    "    gt_d['image_id'] = d[\"img_id\"]\n",
    "    temp[\"caption\"] = d[\"generated_output\"]\n",
    "    gt_d[\"caption\"] = d[\"captions\"]\n",
    "    GROUND_TRUTH.append(gt_d)\n",
    "    LIST_right_format.append(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./new_format_json.json\", 'w') as f:\n",
    "    json.dump(LIST_right_format, f)\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./GROUND_TRUTH_JSON.json\", 'w') as f:\n",
    "    json.dump(GROUND_TRUTH, f)\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pycocotools.coco import COCO\n",
    "# Import COCOResult for the generated captions\n",
    "# from pycocotools.cocoeval import COCOResult \n",
    "from pycocoevalcap.eval import COCOEvalCap\n",
    "from pycocoevalcap.bleu.bleu import Bleu\n",
    "from pycocoevalcap.rouge.rouge import Rouge\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "\n",
    "def calculate_metrics_no_java_v2(gt_json_path, results_json_path):\n",
    "    \"\"\"\n",
    "    Calculates COCO captioning metrics (BLEU, ROUGE-L, CIDEr) by explicitly \n",
    "    loading and formatting both GT and results for robustness.\n",
    "\n",
    "    Args:\n",
    "        gt_json_path (str): Path to the COCO Ground Truth JSON file.\n",
    "        results_json_path (str): Path to the Generated Captions JSON file.\n",
    "    \"\"\"\n",
    "    print(\"--- Loading Ground Truth (GT) Data ---\")\n",
    "    # Initialize COCO for ground truth annotations (This should work fine)\n",
    "    # The COCO constructor expects the path to the GT annotations file\n",
    "    cocoGt = COCO(gt_json_path)\n",
    "    \n",
    "    # --- Load and Format Generated Captions ---\n",
    "    print(\"--- Loading Generated Captions ---\")\n",
    "    # 1. Load the generated results JSON (a list of dictionaries)\n",
    "    with open(results_json_path, 'r') as f:\n",
    "        # Load your generated captions list: [{\"image_id\": id, \"caption\": \"...\"}]\n",
    "        results_list = json.load(f)\n",
    "\n",
    "    # 2. Initialize the COCOResult object using the list.\n",
    "    # We pass the COCO GT object (cocoGt) and the loaded results list (results_list).\n",
    "    # This ensures the results are correctly associated with the GT images and format.\n",
    "    # Note: If you don't have COCOResult, you can try passing the path string again.\n",
    "    try:\n",
    "        cocoRes = cocoGt.loadRes(results_json_path)\n",
    "        # If loadRes fails, fall back to the path string (older pycocotools)\n",
    "    except:\n",
    "        cocoRes = results_json_path\n",
    "\n",
    "\n",
    "    # --- Initialize COCOEvalCap ---\n",
    "    # The standard way to initialize COCOEvalCap is with the GT object and the path/result object.\n",
    "    # Since loadRes often returns the path string back, we should check which is needed.\n",
    "    \n",
    "    # Let's try the most robust way based on typical usage:\n",
    "    print(\"--- Initializing Evaluation ---\")\n",
    "    coco_eval = COCOEvalCap(cocoGt, cocoRes)\n",
    "\n",
    "    # --- Metrics Setup (No Java) ---\n",
    "    coco_eval.evaluators = [\n",
    "        (str('Bleu'), Bleu(4)), \n",
    "        (str('Rouge'), Rouge()),\n",
    "        (str('Cider'), Cider()),\n",
    "    ]\n",
    "    \n",
    "    print(f\"Starting evaluation (excluding METEOR/SPICE) on {len(cocoRes.imgs)} images...\")\n",
    "    \n",
    "    # Perform the evaluation\n",
    "    coco_eval.evaluate()\n",
    "\n",
    "    # Print the resulting scores\n",
    "    print(\"\\n--- Evaluation Scores (No Java Required) ---\")\n",
    "    \n",
    "    for metric, score in coco_eval.eval.items():\n",
    "        print(f\"{metric}: {score:.3f}\")\n",
    "        \n",
    "    return coco_eval.eval\n",
    "\n",
    "# --- Example Configuration (Update these paths) ---\n",
    "GT_JSON_FILE = '/Users/jagathkumarreddyk/Documents/GitHub/BLIP/annotations_trainval2017/annotations/captions_val2017.json'\n",
    "RESULTS_JSON_FILE = './new_format_json.json' # Ensure this path is correct\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "calculate_metrics_no_java_v2(GT_JSON_FILE, RESULTS_JSON_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Madman Protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pycocotools.coco import COCO\n",
    "from pycocoevalcap.eval import COCOEvalCap\n",
    "from pycocoevalcap.bleu.bleu import Bleu\n",
    "from pycocoevalcap.rouge.rouge import Rouge\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "\n",
    "# --- Configuration (Keep your paths here) ---\n",
    "GT_JSON_FILE = '/Users/jagathkumarreddyk/Documents/GitHub/BLIP/annotations_trainval2017/annotations/captions_val2017.json'\n",
    "RESULTS_JSON_FILE = './new_format_json.json' # Ensure this path is correct\n",
    "\n",
    "def calculate_metrics_no_java_fixed(gt_json_path, results_json_path):\n",
    "    \"\"\"\n",
    "    Calculates COCO captioning metrics (BLEU, ROUGE-L, CIDEr) by completely \n",
    "    bypassing the initialization of Java-dependent scorers (METEOR, SPICE).\n",
    "    \"\"\"\n",
    "    print(\"--- Loading Ground Truth (GT) Data ---\")\n",
    "    cocoGt = COCO(gt_json_path)\n",
    "    \n",
    "    # Load and format the generated results\n",
    "    print(\"--- Loading Generated Captions ---\")\n",
    "    \n",
    "    # Use loadRes to associate results with GT image IDs\n",
    "    # If the file load fails here, you'll get a COCO API error, not BrokenPipe\n",
    "    try:\n",
    "        cocoRes = cocoGt.loadRes(results_json_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading results file into COCO format. Check path/format: {e}\")\n",
    "        return {} # Exit function\n",
    "        \n",
    "    # --- Custom Initialize COCOEvalCap ---\n",
    "    coco_eval = COCOEvalCap(cocoGt, cocoRes)\n",
    "\n",
    "    # 1. Define ONLY the Python-based scorers you want to use\n",
    "    scorer_objects = [\n",
    "        (Bleu(4), [\"Bleu_1\", \"Bleu_2\", \"Bleu_3\", \"Bleu_4\"]),\n",
    "        (Rouge(), [\"ROUGE_L\"]),\n",
    "        (Cider(), [\"CIDEr\"])\n",
    "        # DO NOT INCLUDE: Meteor(), Spice()\n",
    "    ]\n",
    "\n",
    "    # 2. Manually set the internal lists expected by COCOEvalCap\n",
    "    coco_eval.evalImgs = []\n",
    "    coco_eval.eval = {}\n",
    "    \n",
    "    # 3. Populate scorers and method names\n",
    "    coco_eval.scorers = []\n",
    "    coco_eval.method = []\n",
    "    \n",
    "    for scorer, method in scorer_objects:\n",
    "        coco_eval.scorers.append(scorer)\n",
    "        coco_eval.method.append(method)\n",
    "\n",
    "    # --- Run Evaluation ---\n",
    "    print(f\"Starting evaluation (metrics: BLEU, ROUGE-L, CIDEr)...\")\n",
    "    \n",
    "    # The default evaluate() method now only iterates over the defined scorers\n",
    "    coco_eval.evaluate()\n",
    "\n",
    "    # Print the resulting scores\n",
    "    print(\"\\n--- Evaluation Scores (No Java Required) ---\")\n",
    "    \n",
    "    # Ensure scores are collected from the new structure\n",
    "    final_scores = {}\n",
    "    for method_list, score_list in zip(coco_eval.method, coco_eval.eval_obj):\n",
    "        if not isinstance(method_list, list):\n",
    "             method_list = [method_list]\n",
    "             score_list = [score_list]\n",
    "             \n",
    "        for method, score in zip(method_list, score_list):\n",
    "            final_scores[method] = score\n",
    "\n",
    "    # Print results\n",
    "    for metric, score in final_scores.items():\n",
    "        print(f\"{metric}: {score:.3f}\")\n",
    "        \n",
    "    return final_scores\n",
    "\n",
    "# --- Execution ---\n",
    "if __name__ == '__main__':\n",
    "    # Make sure to set your paths correctly here\n",
    "    results = calculate_metrics_no_java_fixed(GT_JSON_FILE, RESULTS_JSON_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading Ground Truth (GT) Data ---\n",
      "loading annotations into memory...\n",
      "Done (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "--- Loading Generated Captions ---\n",
      "Loading and preparing results...\n",
      "DONE (t=0.04s)\n",
      "creating index...\n",
      "index created!\n",
      "Loaded 5000 images for evaluation.\n",
      "\n",
      "--- Computing Core Metrics ---\n",
      "Computing Bleu score...\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 86\u001b[39m\n\u001b[32m     82\u001b[39m RESULTS_JSON_FILE = \u001b[33m'\u001b[39m\u001b[33m./new_format_json.json\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;66;03m# Ensure this path is correct\u001b[39;00m\n\u001b[32m     84\u001b[39m \u001b[38;5;66;03m# if __name__ == '__main__':\u001b[39;00m\n\u001b[32m     85\u001b[39m \u001b[38;5;66;03m#     # Ensure your paths are correct before running!\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m results = \u001b[43mcalculate_metrics_from_scratch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mGT_JSON_FILE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mRESULTS_JSON_FILE\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 64\u001b[39m, in \u001b[36mcalculate_metrics_from_scratch\u001b[39m\u001b[34m(gt_json_path, results_json_path)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mComputing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscorer.method()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m score...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# Compute the score: score is the average, scores is per-image list\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m score_list, scores_per_image = \u001b[43mscorer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[38;5;66;03m# Handle single vs. multiple scores (BLEU returns 4 scores)\u001b[39;00m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(score_list, \u001b[38;5;28mlist\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/ICapGPT/.venv/lib/python3.14/site-packages/pycocoevalcap/bleu/bleu.py:33\u001b[39m, in \u001b[36mBleu.compute_score\u001b[39m\u001b[34m(self, gts, res, verbose)\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Sanity check.\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m(\u001b[38;5;28mtype\u001b[39m(hypo) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mlist\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m(\u001b[38;5;28mlen\u001b[39m(hypo) == \u001b[32m1\u001b[39m)\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m(\u001b[38;5;28mtype\u001b[39m(ref) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mlist\u001b[39m)\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m(\u001b[38;5;28mlen\u001b[39m(ref) >= \u001b[32m1\u001b[39m)\n",
      "\u001b[31mAssertionError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pycocotools.coco import COCO\n",
    "from pycocoevalcap.bleu.bleu import Bleu\n",
    "from pycocoevalcap.rouge.rouge import Rouge\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "# You can remove the imports for Meteor and Spice, as they aren't used here.\n",
    "\n",
    "def calculate_metrics_from_scratch(gt_json_path, results_json_path):\n",
    "    \"\"\"\n",
    "    Calculates BLEU-4, ROUGE-L, and CIDEr by initializing the scorers directly.\n",
    "\n",
    "    Args:\n",
    "        gt_json_path (str): Path to the COCO Ground Truth JSON file.\n",
    "        results_json_path (str): Path to the Generated Captions JSON file.\n",
    "    \"\"\"\n",
    "    # 1. Load Data into COCO Objects\n",
    "    print(\"--- Loading Ground Truth (GT) Data ---\")\n",
    "    cocoGt = COCO(gt_json_path)\n",
    "    \n",
    "    print(\"--- Loading Generated Captions ---\")\n",
    "    # This step associates the results with the GT image IDs and prepares them.\n",
    "    try:\n",
    "        cocoRes = cocoGt.loadRes(results_json_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading results file into COCO format. Check path/format: {e}\")\n",
    "        return {} \n",
    "\n",
    "    # 2. Structure Data for Scorers\n",
    "    # The scorers expect two dictionaries mapped by image ID:\n",
    "    # gts: {img_id: [ref1, ref2, ref3, ref4, ref5]}\n",
    "    # res: {img_id: [generated_caption]}\n",
    "    \n",
    "    gts = {} # Ground Truths\n",
    "    res = {} # Results (Generated)\n",
    "    img_ids = cocoGt.getImgIds()\n",
    "    \n",
    "    # Filter the IDs present in the results file to prevent errors\n",
    "    img_ids_to_evaluate = list(cocoRes.imgs.keys()) \n",
    "    \n",
    "    for img_id in img_ids_to_evaluate:\n",
    "        # Get all 5 reference captions for the image\n",
    "        gts[img_id] = [ann['caption'] for ann in cocoGt.imgToAnns[img_id]]\n",
    "        \n",
    "        # Get the single generated caption\n",
    "        res[img_id] = [ann['caption'] for ann in cocoRes.imgToAnns[img_id]]\n",
    "    \n",
    "    print(f\"Loaded {len(img_ids_to_evaluate)} images for evaluation.\")\n",
    "\n",
    "    # 3. Define Scorers to Run\n",
    "    scorers = [\n",
    "        (Bleu(4), [\"Bleu_1\", \"Bleu_2\", \"Bleu_3\", \"Bleu_4\"]),\n",
    "        (Rouge(), [\"ROUGE_L\"]),\n",
    "        (Cider(), [\"CIDEr\"]),\n",
    "    ]\n",
    "    \n",
    "    final_scores = {}\n",
    "    print(\"\\n--- Computing Core Metrics ---\")\n",
    "    \n",
    "    # 4. Compute Scores for Each Metric\n",
    "    for scorer, method_names in scorers:\n",
    "        print(f\"Computing {scorer.method()} score...\")\n",
    "        \n",
    "        # Compute the score: score is the average, scores is per-image list\n",
    "        score_list, scores_per_image = scorer.compute_score(gts, res)\n",
    "        \n",
    "        # Handle single vs. multiple scores (BLEU returns 4 scores)\n",
    "        if isinstance(score_list, list):\n",
    "            for method, score in zip(method_names, score_list):\n",
    "                final_scores[method] = score\n",
    "        else:\n",
    "            final_scores[method_names[0]] = score_list\n",
    "\n",
    "    # 5. Print Results\n",
    "    print(\"\\n--- Evaluation Scores ---\")\n",
    "    for metric, score in final_scores.items():\n",
    "        print(f\"{metric}: {score:.3f}\")\n",
    "        \n",
    "    return final_scores\n",
    "\n",
    "# --- Configuration & Execution ---\n",
    "GT_JSON_FILE = '/Users/jagathkumarreddyk/Documents/GitHub/BLIP/annotations_trainval2017/annotations/captions_val2017.json'\n",
    "RESULTS_JSON_FILE = './new_format_json.json' # Ensure this path is correct\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     # Ensure your paths are correct before running!\n",
    "results = calculate_metrics_from_scratch(GT_JSON_FILE, RESULTS_JSON_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets try to load `model-flickr.pt` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "model_flickr = './model_flickr.pt'\n",
    "\n",
    "# save_path = os.path.join(checkpoint_dir, \"model.pt\")\n",
    "# torch.save({\n",
    "#     \"epoch\": epoch,\n",
    "#     \"q_former\": q_former.state_dict(),\n",
    "#     \"gpt2\": gpt2.state_dict(),\n",
    "#     \"optimizer\": optimizer.state_dict(),\n",
    "#     \"loss\": avg_loss\n",
    "# }, save_path)\n",
    "\n",
    "\n",
    "checkpoint_obj = torch.load(model_flickr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2.load_state_dict(checkpoint_obj[\"gpt2\"])\n",
    "q_former.load_state_dict(checkpoint_obj[\"q_former\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = torch.randn((2,244,244))\n",
    "x = image.reshape([1,*list(image.shape)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 244, 244])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
